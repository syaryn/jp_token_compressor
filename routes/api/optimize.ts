import { Handler } from "$fresh/server.ts";
import { tokenize } from "wakachigaki";
import { getEncoding } from "js-tiktoken";

interface SynonymMap {
  [key: string]: string;
}

const synonymMap: SynonymMap = {};
let isLoaded = false;

// GPT-4oã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’åˆæœŸåŒ–
const encoder = getEncoding("o200k_base");

// å®Ÿéš›ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆæ¸¬
function getTokenCount(text: string): number {
  return encoder.encode(text).length;
}

// æ—¥æœ¬èªæ–‡å­—ã‹ã©ã†ã‹ã‚’åˆ¤å®š
function isJapanese(text: string): boolean {
  return /^[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF\u3400-\u4DBF]+$/.test(text);
}

// è‹±èªï¼ˆã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆï¼‰ã‹ã©ã†ã‹ã‚’åˆ¤å®š
function isAlphabet(text: string): boolean {
  return /^[a-zA-Z]+$/.test(text);
}

// æœ€é©åŒ–ãŒæœ‰åŠ¹ã‹ã©ã†ã‹ã‚’åˆ¤å®š
function shouldOptimize(original: string, optimized: string): boolean {
  // è‹±èªã¸ã®å¤‰æ›ã¯ç¦æ­¢
  if (isJapanese(original) && isAlphabet(optimized)) {
    return false;
  }

  // ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã‹ã‚‰æ—¥æœ¬èªã¸ã®å¤‰æ›ã‚‚ç¦æ­¢
  if (isAlphabet(original) && isJapanese(optimized)) {
    return false;
  }

  const originalTokens = getTokenCount(original);
  const optimizedTokens = getTokenCount(optimized);

  // ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒæ¸›ã‚‰ãªã„å ´åˆã¯å¤‰æ›ã—ãªã„
  if (optimizedTokens >= originalTokens) {
    return false;
  }

  // ååˆ†ãªãƒˆãƒ¼ã‚¯ãƒ³å‰Šæ¸›åŠ¹æœãŒã‚ã‚‹å ´åˆã®ã¿å¤‰æ›ï¼ˆ20%ä»¥ä¸Šå‰Šæ¸›ï¼‰
  const reductionRate = (originalTokens - optimizedTokens) / originalTokens;
  return reductionRate >= 0.2;
}

async function loadSynonymDict(): Promise<void> {
  if (isLoaded) return;

  try {
    // ã¾ãšäº‹å‰æ§‹ç¯‰ã•ã‚ŒãŸè¾æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è©¦ã™
    try {
      console.log("ğŸ“š äº‹å‰æ§‹ç¯‰ã•ã‚ŒãŸè¾æ›¸ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...");
      const prebuiltResponse = await fetch("/synonym-dict.json");
      if (prebuiltResponse.ok) {
        const prebuiltData = await prebuiltResponse.json();
        Object.assign(synonymMap, prebuiltData);
        isLoaded = true;
        console.log(
          `âœ… äº‹å‰æ§‹ç¯‰è¾æ›¸ã‹ã‚‰ ${
            Object.keys(synonymMap).length
          } å€‹ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’é«˜é€Ÿèª­ã¿è¾¼ã¿`,
        );
        return;
      }
    } catch (_prebuiltError) {
      console.log("âš ï¸ äº‹å‰æ§‹ç¯‰è¾æ›¸ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ§‹ç¯‰ã‚’å®Ÿè¡Œ");
    }

    // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»æ§‹ç¯‰
    console.log("ğŸ“¥ Sudachiè¾æ›¸ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...");
    const response = await fetch(
      "https://raw.githubusercontent.com/WorksApplications/SudachiDict/refs/heads/develop/src/main/text/synonyms.txt",
    );
    const text = await response.text();

    // åŒç¾©èªã‚°ãƒ«ãƒ¼ãƒ—ã‚’IDã§ç®¡ç†
    const synonymGroups: { [id: string]: string[] } = {};

    const lines = text.split("\n");
    for (const line of lines) {
      if (line.trim() && !line.startsWith("#")) {
        const parts = line.split(",");
        if (parts.length >= 9) {
          const groupId = parts[0]; // åŒç¾©èªã‚°ãƒ«ãƒ¼ãƒ—ã®ID
          const word = parts[8]; // å˜èªï¼ˆ9ç•ªç›®ã®è¦ç´ ï¼‰

          if (word && word.trim()) {
            const cleanWord = word.trim();
            if (!synonymGroups[groupId]) {
              synonymGroups[groupId] = [];
            }
            synonymGroups[groupId].push(cleanWord);
          }
        }
      }
    }

    // å„ã‚°ãƒ«ãƒ¼ãƒ—ã§æœ€ã‚‚ãƒˆãƒ¼ã‚¯ãƒ³åŠ¹ç‡ã®è‰¯ã„å˜èªã‚’è¦‹ã¤ã‘ã¦ã€ä»–ã®å˜èªã‚’ãƒãƒƒãƒ”ãƒ³ã‚°
    for (const words of Object.values(synonymGroups)) {
      if (words.length > 1) {
        // å®Ÿéš›ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã§æœ€ã‚‚åŠ¹ç‡çš„ãªå˜èªã‚’é¸æŠ
        const mostEfficient = words.reduce((a, b) => {
          const tokensA = getTokenCount(a);
          const tokensB = getTokenCount(b);
          // ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒå°‘ãªã„æ–¹ã‚’é¸æŠã€åŒã˜å ´åˆã¯æ–‡å­—æ•°ãŒå°‘ãªã„æ–¹
          return tokensA < tokensB ||
              (tokensA === tokensB && a.length < b.length)
            ? a
            : b;
        });

        // ã‚°ãƒ«ãƒ¼ãƒ—å†…ã®ä»–ã®å˜èªã‚’æœ€åŠ¹ç‡å˜èªã«ãƒãƒƒãƒ”ãƒ³ã‚°
        for (const word of words) {
          if (word !== mostEfficient && shouldOptimize(word, mostEfficient)) {
            synonymMap[word] = mostEfficient;
          }
        }
      }
    }

    isLoaded = true;
    console.log(
      `Loaded ${Object.keys(synonymMap).length} synonym mappings from ${
        Object.keys(synonymGroups).length
      } groups`,
    );
  } catch (error) {
    console.error("Failed to load synonym dictionary:", error);
  }
}

function optimizeText(text: string): string {
  // å˜èªåˆ†å‰²
  const tokens = tokenize(text);

  // å„å˜èªã‚’åŒç¾©èªè¾æ›¸ã§ç½®ãæ›ãˆ
  const optimizedTokens = tokens.map((token: string) => {
    return synonymMap[token] || token;
  });

  return optimizedTokens.join("");
}

export const handler: Handler = async (req) => {
  if (req.method !== "POST") {
    return new Response("Method not allowed", { status: 405 });
  }

  try {
    // åŒç¾©èªè¾æ›¸ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆåˆå›ã®ã¿ï¼‰
    await loadSynonymDict();

    const body = await req.json();
    const { text } = body;

    if (!text) {
      return new Response(JSON.stringify({ error: "Text is required" }), {
        status: 400,
        headers: { "Content-Type": "application/json" },
      });
    }

    const optimizedText = optimizeText(text);

    return new Response(
      JSON.stringify({
        original: text,
        optimized: optimizedText,
        tokenCount: {
          original: getTokenCount(text),
          optimized: getTokenCount(optimizedText),
        },
      }),
      {
        headers: { "Content-Type": "application/json" },
      },
    );
  } catch (error) {
    console.error("Error processing text:", error);
    return new Response(JSON.stringify({ error: "Internal server error" }), {
      status: 500,
      headers: { "Content-Type": "application/json" },
    });
  }
};
